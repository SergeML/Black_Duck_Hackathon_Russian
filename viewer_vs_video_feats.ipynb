{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":9504813,"datasetId":5783726,"databundleVersionId":9716866}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Подготовка датасета для обучения модели","metadata":{"id":"E3Tat_RzsIbe"}},{"cell_type":"code","source":"!pip install loguru","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbUWPHcGQc_D","outputId":"297d39cb-dc6b-41f7-a9a6-4e990dd77df2","execution":{"iopub.status.busy":"2024-09-29T02:36:31.908187Z","iopub.execute_input":"2024-09-29T02:36:31.908714Z","iopub.status.idle":"2024-09-29T02:36:43.218067Z","shell.execute_reply.started":"2024-09-29T02:36:31.908676Z","shell.execute_reply":"2024-09-29T02:36:43.217044Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: loguru in /opt/conda/lib/python3.10/site-packages (0.7.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom loguru import logger as logging\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom tqdm import tqdm\ntqdm.pandas()\n\ntry:\n  from google.colab import drive\n  drive.mount('/content/drive/')\nexcept:\n  pass","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3n4uLtOwsoUr","outputId":"f2e007f5-7c5a-49c1-9d3c-6372f0c867b2","execution":{"iopub.status.busy":"2024-09-29T02:54:49.379328Z","iopub.execute_input":"2024-09-29T02:54:49.379735Z","iopub.status.idle":"2024-09-29T02:54:49.385860Z","shell.execute_reply.started":"2024-09-29T02:54:49.379699Z","shell.execute_reply":"2024-09-29T02:54:49.384980Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/ghghjg'\nPATH_PREP = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2024-09-29T02:36:55.214374Z","iopub.execute_input":"2024-09-29T02:36:55.214786Z","iopub.status.idle":"2024-09-29T02:36:55.219198Z","shell.execute_reply.started":"2024-09-29T02:36:55.214737Z","shell.execute_reply":"2024-09-29T02:36:55.218369Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def fix_name(name):\n    trans = {\n        'jaroslav':'iaroslav',\n        'zabaikal':'transbai',\n        'primorie':'primor',\n        'sebastop':'sevastop',\n        'saratovs':'saratov',\n        'voronezh':'voron'\n    }\n    if name in trans.keys():\n        return(trans[name])\n    return(name)\n\n\ndef clear_region_name(region_name):\n    drop_words = [' kray', ' krai',' oblast', ' obl', ' republic', 'republic of ']+[x for x in '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~’ ']\n    region_name = region_name.lower()\n    for drop_word in drop_words:\n        region_name = region_name.replace(drop_word, '')\n    return(fix_name(region_name[:8].replace('y','i')))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T02:36:56.770374Z","iopub.execute_input":"2024-09-29T02:36:56.771025Z","iopub.status.idle":"2024-09-29T02:36:56.778092Z","shell.execute_reply.started":"2024-09-29T02:36:56.770982Z","shell.execute_reply":"2024-09-29T02:36:56.777171Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class BertTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        tokenizer=os.path.join(PATH_PREP, 'tokenizer'),\n        model=os.path.join(PATH_PREP, 'model'),\n    ):\n        logging.info('Loading tokenizer...')\n        if type(tokenizer) == str:\n          try:\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n          except:\n            logging.info(f'{tokenizer} is not available. Downloading tokenizer...')\n            self.tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n            self.tokenizer.save_pretrained(tokenizer)\n            logging.info('Tokenizer is downloaded.')\n        else:\n          self.tokenizer = tokenizer\n        logging.info('Tokenizer is loaded.')\n\n        logging.info('Loading language model...')\n        if type(model) == str:\n          try:\n            self.model = AutoModel.from_pretrained(model)\n          except:\n            logging.info(f'{model} is not available. Downloading model...')\n            self.model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n            self.model.save_pretrained(model)\n            logging.info('Model is downloaded.')\n        else:\n            self.model = model\n        logging.info('Language model is loaded.')\n        try:\n          self.model.cuda()\n        except:\n          pass\n\n    def embed_bert_cls(self, text: str):\n        model = self.model\n        tokenizer = self.tokenizer\n        t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n        embeddings = model_output.last_hidden_state[:, 0, :]\n        embeddings = torch.nn.functional.normalize(embeddings)\n        return embeddings[0].cpu().numpy()\n\n    def make_emb_feats(self, row):\n        emb = self.embed_bert_cls(row['title'])\n        res = pd.concat([pd.Series(row['rutube_video_id'])] + [pd.Series(e for e in emb)])\n        return res\n\n    def transform(self, text: str):\n        return text.progress_apply(lambda x: self.make_emb_feats(x), axis=1)\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n        return self","metadata":{"id":"rszTfQ-n1Qey","execution":{"iopub.status.busy":"2024-09-29T02:36:58.172145Z","iopub.execute_input":"2024-09-29T02:36:58.172813Z","iopub.status.idle":"2024-09-29T02:36:58.199616Z","shell.execute_reply.started":"2024-09-29T02:36:58.172773Z","shell.execute_reply":"2024-09-29T02:36:58.198801Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Prepeocessor():\n  def __init__(\n      self,\n      video,\n      events,\n  ):\n    self.video = video\n    self.events = events\n    self.videos_for_agg = None\n\n  def gen_videos(self):\n    cols = set(self.video.columns)\n    logging.info('Generating video features...')\n    if os.path.isfile(os.path.join(PATH_PREP, 'videos_for_agg.pq')):\n      self.videos_for_agg = pd.read_parquet(os.path.join(PATH_PREP, 'videos_for_agg.pq'))\n    else:\n      self.videos_for_agg = self.video[['author_id', 'rutube_video_id', 'duration']]\n      self.videos_for_agg.to_parquet(os.path.join(PATH_PREP, 'videos_for_agg.pq'))\n\n    if os.path.isfile(os.path.join(PATH_PREP, 'title_freq.pq')):\n      title_freq = pd.read_parquet(os.path.join(PATH_PREP, 'title_freq.pq'))\n    else:\n      title_freq = self.video.groupby(by='title')['rutube_video_id'].agg('nunique').reset_index(drop=False).rename(columns={'rutube_video_id': 'title_freq'})\n      title_freq.to_parquet(os.path.join(PATH_PREP, 'title_freq.pq'))\n    self.video = self.video.merge(title_freq, how='left', on='title')\n    logging.info('Video features are generated.')\n    logging.info(f'New cols: {\", \".join(set(self.video.columns).difference(cols))}.')\n\n  def gen_events(self):\n    logging.info('Generating events features...')\n    cols = set(self.events.columns)\n    \n    self.events = self.events.sort_values([\"event_timestamp\", \"viewer_uid\"])\n    \n    regions = pd.read_csv(os.path.join(PATH, 'regions_dict.csv'))\n    self.events['region_for_time'] = self.events['region'].apply(clear_region_name)\n    self.events['event_time'] = pd.to_datetime(self.events['event_timestamp'].apply(lambda x: x.replace('+03:00','')))\n    self.events = self.events.merge(regions.rename(columns = {'region':'region_for_time'}), how = 'left', left_on = 'region_for_time', right_on = 'region_for_time')\n    del regions\n    \n    self.events['hours_corrector'] = self.events['hours_corrector'].fillna(0) \n    self.events['local_event_timestamp'] = self.events['event_time'] + self.events['hours_corrector'].astype('timedelta64[h]')\n    self.events['weekday'] = self.events['local_event_timestamp'].dt.weekday\n    self.events['hour'] = self.events['local_event_timestamp'].dt.hour\n    self.events = self.events.drop(columns=['hours_corrector', 'region_for_time', 'local_event_timestamp'])\n    \n    self.events['event_timestamp'] = pd.to_datetime(self.events['event_timestamp'])\n    self.events['time_diff'] = self.events.groupby('viewer_uid')['event_timestamp'].diff().dt.total_seconds() / 60\n    self.events['time_diff2'] = self.events.groupby('viewer_uid')['time_diff'].shift(-1)\n    self.events['time_diff2'] = self.events['time_diff2'] - (self.events['total_watchtime'] / 60)\n    self.events['time_diff'] = self.events.groupby('viewer_uid')['time_diff2'].shift(1)\n    self.events['new_session'] = (self.events['time_diff'] > 120) | self.events['time_diff'].isna()\n    self.events['session_id'] = self.events.groupby('viewer_uid')['new_session'].cumsum() \n\n    self.events['ua_device_type_set'] = self.events['ua_device_type'].apply(lambda x: set([x]))\n    self.events['ua_device_type_set_diff'] = self.events.groupby('viewer_uid')['ua_device_type_set'].diff()\n    self.events['ua_device_type_set_diff'] = self.events['ua_device_type_set_diff'].apply(lambda x: {} if pd.isna(x) else x)\n    self.events['ua_device_type_set_diff'] = self.events['ua_device_type_set_diff'].apply(lambda x: np.NaN if len(x) == 0 else 1)\n\n    self.events['ua_client_type_set'] = self.events['ua_client_type'].apply(lambda x: set([x]))\n    self.events['ua_client_type_set_diff'] = self.events.groupby('viewer_uid')['ua_client_type_set'].diff()\n    self.events['ua_client_type_set_diff'] = self.events['ua_client_type_set_diff'].apply(lambda x: {} if pd.isna(x) else x)\n    self.events['ua_client_type_set_diff'] = self.events['ua_client_type_set_diff'].apply(lambda x: np.NaN if len(x) == 0 else 1)\n\n    self.events['ua_client_name_set'] = self.events['ua_client_name'].apply(lambda x: set([x]))\n    self.events['ua_client_name_set_diff'] = self.events.groupby('viewer_uid')['ua_client_name_set'].diff()\n    self.events['ua_client_name_set_diff'] = self.events['ua_client_name_set_diff'].apply(lambda x: {} if pd.isna(x) else x)\n    self.events['ua_client_name_set_diff'] = self.events['ua_client_name_set_diff'].apply(lambda x: np.NaN if len(x) == 0 else 1)\n    self.events = self.events.drop(['ua_client_type_set', 'ua_device_type_set', 'ua_client_name_set'], axis=1)\n    self.events[[\n        'ua_device_type_set_diff', 'ua_client_type_set_diff',\n        'ua_client_name_set_diff', 'time_diff', 'time_diff2'\n    ]] = self.events[[\n        'ua_device_type_set_diff', 'ua_client_type_set_diff',\n        'ua_client_name_set_diff', 'time_diff', 'time_diff2']].fillna(0)\n    \n    if os.path.isfile(os.path.join(PATH_PREP, 'first_known_view_timelag.pq')):\n      first_known_view_timelag = pd.read_parquet(os.path.join(PATH_PREP, 'first_known_view_timelag.pq'))\n    else:\n      first_known_view_timelag = self.events.groupby(by='rutube_video_id')['event_timestamp'].agg('min').reset_index(drop=False).rename(columns={'event_timestamp': 'timelag'})\n      first_known_view_timelag.to_parquet(os.path.join(PATH_PREP, 'first_known_view_timelag.pq'))\n    self.events = self.events.merge(first_known_view_timelag, how='left', on='rutube_video_id')\n    del first_known_view_timelag\n    self.events['timelag'] = (pd.to_datetime(self.events['event_timestamp']) - pd.to_datetime(self.events['timelag'])).dt.total_seconds()\n\n    if os.path.isfile(os.path.join(PATH_PREP, 'video_id_freq.pq')):\n      video_id_freq = pd.read_parquet(os.path.join(PATH_PREP, 'video_id_freq.pq'))\n    else:\n      video_id_freq = (self.events.groupby(by='rutube_video_id')['viewer_uid'].agg('count') / self.events.rutube_video_id.nunique()).reset_index(drop=False).rename(columns={'viewer_uid': 'video_id_freq'})\n      video_id_freq.to_parquet(os.path.join(PATH_PREP, 'video_id_freq.pq'))\n    self.events = self.events.merge(video_id_freq, how='left', on='rutube_video_id') \n    \n    logging.info('Events features are generated.')\n    logging.info(f'New cols: {\", \".join(set(self.events.columns).difference(cols))}.')\n\n  def gen_mixed(self):\n    logging.info('Generating mixed features...')\n    cols = set(self.events.columns)\n    if os.path.isfile(os.path.join(PATH_PREP, 'author_id_freq.pq')):\n      author_id_freq = pd.read_parquet(os.path.join(PATH_PREP, 'author_id_freq.pq'))\n    else:\n      author_id_freq = (self.events.merge(\n          self.videos_for_agg, how='left', on='rutube_video_id'\n          ).groupby(by='author_id')['viewer_uid'].agg('count') / len(self.events)).reset_index(drop=False).rename(columns={'viewer_uid': 'author_id_freq'})\n      author_id_freq.to_parquet(os.path.join(PATH_PREP, 'author_id_freq.pq'))\n    self.events = self.events.merge(self.videos_for_agg, how='left', on='rutube_video_id')\n    self.events = self.events.merge(author_id_freq, how='left', on='author_id').drop(columns='author_id')\n    self.events['frac_watch'] = self.events['total_watchtime'] * 60 / self.events['duration']\n    del author_id_freq\n    logging.info('Mixed features are generated.')\n    logging.info(f'New cols: {\", \".join(set(self.events.columns).difference(cols))}.')","metadata":{"id":"E_zeNTdl7oW2","execution":{"iopub.status.busy":"2024-09-29T02:54:56.092558Z","iopub.execute_input":"2024-09-29T02:54:56.093079Z","iopub.status.idle":"2024-09-29T02:54:56.128738Z","shell.execute_reply.started":"2024-09-29T02:54:56.093042Z","shell.execute_reply":"2024-09-29T02:54:56.127996Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Чтение данных и сбор датасета","metadata":{"id":"QH7BHKlIsO4t"}},{"cell_type":"code","source":"def join_csvs(path_events: [list[str], str] = [\n    os.path.join(PATH, 'all_events.csv'),\n    os.path.join(PATH, 'train_events.csv')\n                                          ],\n              path_videos: [list[str], str] = [\n    os.path.join(PATH, 'video_info_v2.csv'),\n                                          ],\n              saving_path: [None, str] = None,\n              make_title_emb: bool = False) -> pd.DataFrame:\n  \"\"\"\n  Функция join_csvs возвращает DataFrame с объединенными данными о событиях\n  и известными данными о видео, упоминаемых в данных с событиями. Информация\n  о видео присоединяется к событиям слева по ключу `rutube_video_id`.\n\n  Аргументы\n  ----------\n  path_events: list, str, default = [\n    '/content/drive/MyDrive/Colab Notebooks/rutube/datasets/all_events.csv',\n    '/content/drive/MyDrive/Colab Notebooks/rutube/datasets/train_events.csv'\n                                      ]\n      Строка или список строк, содержащие пути до csv-файлов с логами событий;\n  path_videos: list, str, default =\n    '/content/drive/MyDrive/Colab Notebooks/rutube/datasets/video_info_v2.csv'\n      Строка или список строк, содержащие пути до csv-файлов с описанием видео;\n\n  saving_path: None, str, default = None\n      Строка, содержащая путь для записи csv-файла с итоговой таблицей. Если\n      None, записи не происходит.\n\n  Возвращает\n  ----------\n  data: pd.DataFrame\n      Таблица, содержащая объединенные данные по всем файлам.\n  \"\"\"\n\n  # инициализируем датасеты\n\n  # если путь строка, то читаем csv-файл по пути, указанному в строке\n  logging.info('Loading events datasets...')\n  if type(path_events) == str:\n    events = pd.read_csv(path_events)\n  # иначе объединяем таблицы, считанные из списка строк с путями\n  else:\n    events = pd.concat([pd.read_csv(x) for x in path_events])\n  logging.info('Events datasets are loaded.')\n\n  if type(path_videos) == str:\n    logging.info('Loading video info datasets...')\n    videos = pd.read_csv(path_videos)\n  else:\n    videos = pd.concat([pd.read_csv(x) for x in path_videos])\n  logging.info('Videos info datasets are loaded.')\n\n  logging.info('Extracting features...')\n  p = Prepeocessor(videos, events)\n  p.gen_events()\n  p.gen_videos()\n  p.gen_mixed()\n  events = p.events\n  logging.info('Features are extracted.')\n\n\n\n  # формирование эмбеддингов видео\n  if make_title_emb:\n    logging.info('Cooking videos titles embeddings...')\n    bert_dicti = pd.DataFrame()\n    text_vectorizer = BertTransformer()\n    bert_dicti['title'] = pd.Series(videos.title.unique())\n    bert_dicti = videos[['title', 'rutube_video_id', 'category']].merge(\n        bert_dicti, how='left', on='title'\n        )\n    bert_dicti = pd.concat([bert_dicti, BertTransformer().fit_transform(bert_dicti)], axis=1)\n    bert_dicti.columns = ['title', 'rutube_video_id', 'category'] + [f'e_{i}' for i in range(1, bert_dicti.shape[1])]\n    logging.info('Videos titles embeddings are ready.')\n\n  if make_title_emb:\n    logging.info('Merging embeddings...')\n    # присоединяем по ключу к событиям эмбеддинги описаний видео, упомянутых в них\n    #events = (pd.read_csv(os.path.join(PATH, 'train_events.csv'))[['viewer_uid', 'rutube_video_id']]).merge(events, on=['viewer_uid', 'rutube_video_id'], how='left')\n    events = events.merge(bert_dicti, how='left', on='rutube_video_id')\n    logging.info('Data is merged.')\n\n  # сохраняем итоговую таблицу в csv-файл, если указан путь для сохранения\n  if saving_path:\n    logging.info('Saving prepared table as csv...')\n    events.to_csv(saving_path)\n    logging.info('csv-file is saved.')\n  return events","metadata":{"id":"BvmYb0prsCQR","execution":{"iopub.status.busy":"2024-09-29T02:55:06.784209Z","iopub.execute_input":"2024-09-29T02:55:06.785073Z","iopub.status.idle":"2024-09-29T02:55:06.799740Z","shell.execute_reply.started":"2024-09-29T02:55:06.785031Z","shell.execute_reply":"2024-09-29T02:55:06.798736Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data = join_csvs(make_title_emb=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFDa_rz46AI3","outputId":"c2d0755a-d622-4891-9a78-be79e99cf8e1","execution":{"iopub.status.busy":"2024-09-29T02:55:09.728997Z","iopub.execute_input":"2024-09-29T02:55:09.729378Z","iopub.status.idle":"2024-09-29T03:02:54.789298Z","shell.execute_reply.started":"2024-09-29T02:55:09.729340Z","shell.execute_reply":"2024-09-29T03:02:54.788362Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"\u001b[32m2024-09-29 02:55:09.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjoin_csvs\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading events datasets...\u001b[0m\n\u001b[32m2024-09-29 02:55:33.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjoin_csvs\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mEvents datasets are loaded.\u001b[0m\n\u001b[32m2024-09-29 02:55:35.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjoin_csvs\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mVideos info datasets are loaded.\u001b[0m\n\u001b[32m2024-09-29 02:55:35.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjoin_csvs\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mExtracting features...\u001b[0m\n\u001b[32m2024-09-29 02:55:35.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_events\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mGenerating events features...\u001b[0m\n\u001b[32m2024-09-29 03:02:38.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_events\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mEvents features are generated.\u001b[0m\n\u001b[32m2024-09-29 03:02:38.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_events\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNew cols: video_id_freq, timelag, weekday, time_diff, area_type, ua_client_type_set_diff, event_time, new_session, hour, time_diff2, ua_device_type_set_diff, ua_client_name_set_diff, session_id.\u001b[0m\n\u001b[32m2024-09-29 03:02:38.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_videos\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mGenerating video features...\u001b[0m\n\u001b[32m2024-09-29 03:02:40.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_videos\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mVideo features are generated.\u001b[0m\n\u001b[32m2024-09-29 03:02:40.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_videos\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mNew cols: title_freq.\u001b[0m\n\u001b[32m2024-09-29 03:02:40.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_mixed\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mGenerating mixed features...\u001b[0m\n\u001b[32m2024-09-29 03:02:54.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_mixed\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mMixed features are generated.\u001b[0m\n\u001b[32m2024-09-29 03:02:54.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgen_mixed\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mNew cols: duration, author_id_freq, frac_watch.\u001b[0m\n\u001b[32m2024-09-29 03:02:54.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mjoin_csvs\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mFeatures are extracted.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"data.to_parquet(os.path.join(PATH_PREP, 'all_features.parquet'))","metadata":{"id":"FNz-6gEgCVWE","execution":{"iopub.status.busy":"2024-09-29T03:02:54.790804Z","iopub.execute_input":"2024-09-29T03:02:54.791112Z","iopub.status.idle":"2024-09-29T03:03:11.951186Z","shell.execute_reply.started":"2024-09-29T03:02:54.791080Z","shell.execute_reply":"2024-09-29T03:03:11.950167Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"data.merge(pd.read_parquet('/kaggle/input/ghghjg/train_targets.pq'), on='viewer_uid', how='right').to_parquet(os.path.join(PATH_PREP, 'train.parquet'))\ndata.merge(pd.read_parquet('/kaggle/input/ghghjg/val_targets.pq'), on='viewer_uid', how='right').to_parquet(os.path.join(PATH_PREP, 'val.parquet'))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T03:04:59.875529Z","iopub.execute_input":"2024-09-29T03:04:59.875925Z","iopub.status.idle":"2024-09-29T03:05:06.655409Z","shell.execute_reply.started":"2024-09-29T03:04:59.875888Z","shell.execute_reply":"2024-09-29T03:05:06.654597Z"},"trusted":true},"execution_count":28,"outputs":[]}]}